{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rl6PZF745OO"
      },
      "source": [
        "# Chapter 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAwD-RMf4-Ng"
      },
      "source": [
        "## Question 2.1\n",
        "In this problem, we consider two ways of writing the systematic component of a simple linear regression model.\n",
        "1. Interpret the meaning of the constant term $β_0$ when the systematic component is written as $μ = β_0 + β_1 x$.\n",
        "2. Interpret the meaning of the constant term $α_0$ when the systematic component is written as $μ = α_0 + β_1(x − \\bar{x})$.\n",
        "\n",
        "### Answers\n",
        "#### 1.\n",
        "Here, $\\beta_0$ is the expected value of $y$, $\\mu$, when $x=0$.  You can see this because when $x=0$, then $\\beta_1 x = 0$ such that $\\mu = \\beta_0 + 0 = \\beta_0$.\n",
        "\n",
        "#### 2.\n",
        "Here, $\\alpha_0$ is the expected value of $y$, $\\mu$, when $x=\\bar{x}$.  You can see this because when $x=\\bar{x}$, then $(x-\\bar{x}) = \\beta_1 (x-\\bar{x}) = 0$ such that $\\mu = \\alpha_0 + 0 = \\alpha_0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJCuHbATw_YY"
      },
      "source": [
        "## Question 2.2\n",
        "For simple linear regression, show that the simultaneous solutions to $∂S/∂β_0 = 0$ and $∂S/∂β_1 = 0$ in (2.4)\n",
        "\n",
        "$$\\frac{\\partial S(\\beta_0, \\beta_1)}{\\partial \\beta_0} = 2 \\sum_{i=1}^n w_i(y_i - \\mu_i)$$\n",
        "\n",
        "and (2.5)\n",
        "\n",
        "$$\\frac{\\partial S(\\beta_0, \\beta_1)}{\\partial \\beta_1} = 2 \\sum_{i=1}^n w_i x_i(y_i - \\mu_i)$$\n",
        "\n",
        "produce the solutions shown in (2.6)\n",
        "\n",
        "$$\\hat{\\beta_0} = \\bar{y}_w - \\hat{\\beta_1}\\bar{x}_w$$\n",
        "\n",
        "and (2.7)\n",
        "\n",
        "$$\\hat{\\beta_1} = \\frac{SS_{xy}}{SS_x} = \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)y_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\bar{y}_w = \\frac{\\sum_{i=1}^n w_i y_i}{\\sum_{i=1}^n w_i}$$\n",
        "\n",
        "(p. 36-37).\n",
        "\n",
        "### Answer\n",
        "Let's remind oursleves of what $S$ is: it's the **Sum of Squares**; our measure of a model's lack of fit, and therefore the quantity we're tyring to *minimize* when fitting a model (hence, the introduction of derivatives).\n",
        "\n",
        "$$S = \\sum_{i=1}^n w_i(y_i - {\\mu}_i)^2 = \\sum_{i=1}^n w_i(y_i - ({\\beta}_0 + {\\beta}_1 x_i))^2$$\n",
        "\n",
        "We get (2.4) and (2.5) by differentiating this expression by $\\beta_0$ and $\\beta_1$, respectively.  Let's expand these expressions so that we can see where $\\beta_0$ and $\\beta_1$ are:\n",
        "\n",
        "$$\\frac{\\partial S(\\beta_0, \\beta_1)}{\\partial \\beta_0} = -2 \\sum_{i=1}^n w_i(y_i - \\mu_i) = -2 \\sum_{i=1}^n w_i(y_i - (\\beta_0 + \\beta_1 x_i))$$\n",
        "\n",
        "$$\\frac{\\partial S(\\beta_0, \\beta_1)}{\\partial \\beta_1} = -2 \\sum_{i=1}^n w_i x_i(y_i - (\\beta_0 + \\beta_1 x_i))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIJwE5kQmhCN"
      },
      "source": [
        "Now, if want to find a minimum (or maximum) for $S$, we need to find the point at which $∂S/∂β_0 = 0$ and $∂S/∂β_1 = 0$, then solve for the coefficients.\n",
        "\n",
        "$$\\frac{\\partial S(\\beta_0, \\beta_1)}{\\partial \\beta_0} = 2 \\sum_{i=1}^n w_i(y_i - (\\beta_0 + \\beta_1 x_i)) = 0$$\n",
        "$$= \\sum_{i=1}^n w_i(y_i - \\beta_0 - \\beta_1 x_i)$$\n",
        "$$= \\sum_{i=1}^n w_i y_i - w_i \\beta_0 - w_i \\beta_1 x_i$$\n",
        "$$\\rightarrow \\sum_{i=1}^n w_i \\beta_0 = \\sum_{i=1}^n w_i y_i - w_i \\beta_1 x_i $$\n",
        "$$= \\beta_0 \\sum_{i=1}^n w_i = \\sum_{i=1}^n w_i y_i - \\beta_1 \\sum_{i=1}^n w_i x_i$$\n",
        "$$\\rightarrow {\\beta}_0 = \\frac{\\sum_{i=1}^n w_i y_i}{\\sum_{i=1}^n w_i} - \\beta_1 \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}$$\n",
        "$$= \\bar{y}_w - {\\beta_1}\\bar{x}_w$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGCsCMQDuY-u"
      },
      "source": [
        "$$\\frac{\\partial S(\\beta_0, \\beta_1)}{\\partial \\beta_1} = 2 \\sum_{i=1}^n w_i x_i(y_i - \\mu_i) = 0 $$\n",
        "$$= \\sum_{i=1}^n w_i x_i(y_i - (\\beta_0 + \\beta_1 x_i))$$\n",
        "$$= \\sum_{i=1}^n w_i x_i(y_i - \\beta_0 - \\beta_1 x_i)$$\n",
        "$$= \\sum_{i=1}^n w_i x_i y_i - \\sum_{i=1}^n w_i x_i \\beta_0 - \\sum_{i=1}^n w_i x_i \\beta_1 x_i$$\n",
        "$$= \\sum_{i=1}^n w_i x_i y_i - \\beta_0 \\sum_{i=1}^n w_i x_i  - \\beta_1 \\sum_{i=1}^n w_i x_i^2$$\n",
        "$$= \\sum_{i=1}^n w_i x_i y_i - (\\bar{y}_w - \\beta_1 \\bar{x}_w) \\sum_{i=1}^n w_i x_i  - \\beta_1 \\sum_{i=1}^n w_i x_i^2$$\n",
        "$$\\rightarrow (\\bar{y}_w - \\beta_1 \\bar{x}_w) \\sum_{i=1}^n w_i x_i  + \\beta_1 \\sum_{i=1}^n w_i x_i^2 = \\sum_{i=1}^n w_i x_i y_i$$\n",
        "$$= \\bar{y}_w \\sum_{i=1}^n w_i x_i - \\beta_1 \\bar{x}_w \\sum_{i=1}^n w_i x_i + \\beta_1 \\sum_{i=1}^n w_i x_i^2 = \\sum_{i=1}^n w_i x_i y_i$$\n",
        "$$= \\beta_1(\\sum_{i=1}^n w_i x_i^2 - \\bar{x}_w \\sum_{i=1}^n w_i x_i) = \\sum_{i=1}^n w_i x_i y_i - \\bar{y}_w \\sum_{i=1}^n w_i x_i$$\n",
        "$$\\rightarrow \\beta_1 = \\frac{\\sum_{i=1}^n w_i x_i y_i - \\bar{y}_w \\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i x_i^2 - \\bar{x}_w \\sum_{i=1}^n w_i x_i}$$\n",
        "\n",
        "So we have an expression for $\\beta_1$, but it doesn't look like the expression we're supposed to derive.  We could take the easy way out and expand the numerator ($\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)y_i$) and denominator ($\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2$) from our desired expression and find that they equal the numerator and denomiator of our current expression, but that seems like cheating.  I want to get from our current expression to the desired expression.\n",
        "\n",
        "For our numerator, let's recall that $\\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}$ and $\\bar{y}_w = \\frac{\\sum_{i=1}^n w_i y_i}{\\sum_{i=1}^n w_i}$ such that $\\bar{x}_w \\sum_{i=1}^n w_i = \\sum_{i=1}^n w_i x_i$ and $\\bar{y}_w \\sum_{i=1}^n w_i = \\sum_{i=1}^n w_i y_i$.  So for our numerator we get\n",
        "\n",
        "\\\n",
        "\n",
        "$$\\sum_{i=1}^n w_i x_i y_i - \\bar{y}_w \\sum_{i=1}^n w_i x_i = \\sum_{i=1}^n w_i x_i y_i - \\bar{y}_w(\\bar{x}_w \\sum_{i=1}^n w_i)\n",
        "= \\sum_{i=1}^n w_i(x_i y_i - \\bar{x}_w \\bar{y}_w)\n",
        "=\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) y_i + \\bar{y}_w \\sum_{i=1}^n w_i (x_i - \\bar{x}_w)$$\n",
        "\n",
        "\\\n",
        "\n",
        "Now, $\\sum_{i=1}^n w_i (x_i - \\bar{x}_w) = 0$, leaving only $\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) y_i$ in the numerator.\n",
        "\n",
        "As for the deonomiator, we need to a summation property.  Namely, $\\sum w_i v_i = \\bar{v}_w \\sum w_i$, where $v$ is a variable like $x$ or $y$.  So let us rewrite our denominator:\n",
        "\n",
        "\\\n",
        "\n",
        "$$\\sum_{i=1}^n w_i x_i^2 - \\bar{x}_w \\sum_{i=1}^n w_i x_i\n",
        "= \\sum_{i=1}^n w_i x_i^2 - \\bar{x}_w \\bar{x}_w \\sum_{i=1}^n w_i\n",
        "= \\sum_{i=1}^n w_i x_i^2 - \\bar{x}_w^2 \\sum_{i=1}^n w_i\n",
        "= \\sum_{i=1}^n w_i(x_i^2 - \\bar{x}_w^2)$$\n",
        "\n",
        "\\\n",
        "\n",
        "Now please allow me this one cheat, expanded our target to show it's equal to what we have:\n",
        "$$\\sum_{i=1}^n w_i (x_i - \\bar{x}_w)^2\n",
        "= \\sum_{i=1}^n w_i (x_i - \\bar{x}_w)(x_i - \\bar{x}_w)\n",
        "= \\sum_{i=1}^n w_i (x_i^2 - 2 x_i \\bar{x}_w + \\bar{x}_w^2)\n",
        "= \\sum_{i=1}^n w_i x_i^2 -2 \\sum_{i=1}^n w_i x_i \\bar{x}_w + \\sum_{i=1}^n w_i \\bar{x}_w^2\n",
        "= \\sum_{i=1}^n w_i x_i^2 - 2 \\bar{x}_w \\sum_{i=1}^n w_i x_i + \\bar{x}_w^2 \\sum_{i=1}^n w_i\n",
        "= \\sum_{i=1}^n w_i x_i^2 - 2 \\bar{x}_w \\bar{x}_w \\sum_{i=1}^n w_i + \\bar{x}_w^2 \\sum_{i=1}^n w_i\n",
        "= \\sum_{i=1}^n w_i x_i^2 - 2 \\bar{x}_w^2 \\sum_{i=1}^n w_i + \\bar{x}_w^2 \\sum_{i=1}^n w_i\n",
        "= \\sum_{i=1}^n w_i x_i^2 - \\bar{x}_w^2 \\sum_{i=1}^n w_i\n",
        "= \\sum_{i=1}^n w_i (x_i^2 - \\bar{x}_w^2)$$\n",
        "\n",
        "\\\n",
        "\n",
        "which is what we initally derived.  So finally, we have\n",
        "$$\\beta_1 = \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)y_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYQ4tRqsEvNU"
      },
      "source": [
        "## Question 2.3\n",
        "In the case of simple linear regression with all weights set to one, show that\n",
        "\n",
        "$$X^T W X = \\begin{bmatrix}\n",
        "n & \\sum{x} \\\\\n",
        "\\sum{x} & \\sum{x^2}\\\\\n",
        "\\end{bmatrix}$$\n",
        "where the summations are over $i = 1,2,...,n$.  Hence, show that\n",
        "$$\\hat{\\beta}_1 = \\frac{\\sum{xy} - \\frac{\\sum{x}\\sum{y}}{n}}{\\sum{x^2} - \\frac{(\\sum{x})^2}{n}}\n",
        "$$\n",
        "\n",
        "### Answer\n",
        "Okay, let's start by defining our matices:\n",
        "\n",
        "$$X = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_n \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$X^T = \\begin{bmatrix}\n",
        "1 & ... & 1 \\\\\n",
        "x_1 & ... & x_n \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$W = \\begin{bmatrix}\n",
        "w_1 & 0 & 0 & ... & 0 \\\\\n",
        "0 & w_2 & 0 & ... & 0 \\\\\n",
        "\\ & \\ & \\ddots & \\ & \\ \\\\\n",
        "0 & ... & ... & ... & w_n \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Now multiply\n",
        "\n",
        "$$X^T W X =\n",
        "\\begin{bmatrix}\n",
        "1 & ... & 1 \\\\\n",
        "x_1 & ... & x_n \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "w_1 & 0 & 0 & ... & 0 \\\\\n",
        "0 & w_2 & 0 & ... & 0 \\\\\n",
        "\\ & \\ & \\ddots & \\ & \\ \\\\\n",
        "0 & ... & ... & ... & w_n \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_n \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "1 & ... & 1 \\\\\n",
        "x_1 & ... & x_n \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "w_1 & w_1 x_1 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "w_n & w_n x_n \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "\\sum_{i=1}^n w_i & \\sum_{i=1}^n w_i x_i \\\\\n",
        "\\sum_{i=1}^n w_i x_i & \\sum_{i=1}^n (w_i x_i)^2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "When all $w_i = 1$, this becomes\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "n & \\sum_{i=1}^n x_i \\\\\n",
        "\\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "as per the first part of the problem.\n",
        "\n",
        "\\\n",
        "\n",
        "Now we must show $\\hat{\\beta}_1 = \\frac{\\sum{xy} - \\frac{\\sum{x}\\sum{y}}{n}}{\\sum{x^2} - \\frac{(\\sum{x})^2}{n}}$.  To start, let's derive the vector for $\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}$.\n",
        "\n",
        "In matrix notation\n",
        "\n",
        "$$\\mu = X \\beta$$\n",
        "\n",
        "such that the deviations from our predicted value and actual values are\n",
        "\n",
        "$$d = y - \\mu = y - X\\beta$$\n",
        "\n",
        "where $\\mu$ and $\\beta$ are vectors of length $n$ (the number of observations) and $p+1$ (2 for simple linear regression, the intercept and slope), respectively, and $X$ is a matrix with $n$ rows and $p+1$ columns.\n",
        "\n",
        "Our weighted sum of squares $S$ becomes\n",
        "\n",
        "$$S = (y - X\\beta)^T W (y - X\\beta)$$\n",
        "$$ = y^T W y - y^T W X\\beta - (X\\beta)^T W y + (X\\beta)^T W X\\beta$$\n",
        "$$ = y^T W y - y^T W X\\beta - \\beta^TX^T W y + \\beta^TX^T W X\\beta$$\n",
        "\n",
        "To find the value for $\\beta$ that minimizes $S$, we need to find $\\frac{dS}{d\\beta}$ and set it to 0.\n",
        "\n",
        "$$\\frac{dS}{d\\beta} = 0 - y^T W X - X^T W y + 2X^T W X\\beta = 2X^T W X\\beta - 2X^T W y =_{set} 0$$\n",
        "$$\\rightarrow X^T W X\\beta = X^T W y$$\n",
        "$$\\hat{\\beta} = (X^T W X)^{-1} X^T W y$$\n",
        "\n",
        "We already have $X^T W X$.  Now we need to find its inverse, as well as $X^T W y$.\n",
        "\n",
        "$$(X^T W X)^{-1} =\n",
        "\\begin{bmatrix}\n",
        "\\sum_{i=1}^n w_i & \\sum_{i=1}^n w_i x_i \\\\\n",
        "\\sum_{i=1}^n w_i x_i & \\sum_{i=1}^n (w_i x_i)^2 \\\\\n",
        "\\end{bmatrix}\n",
        "^{-1}\n",
        "= \\frac{1}{\\sum_{i=1}^n w_i \\sum_{i=1}^n (w_i x_i)^2 - (\\sum_{i=1}^n w_i x_i)^2} \\begin{bmatrix}\n",
        "\\sum_{i=1}^n (w_i x_i)^2 & -\\sum_{i=1}^n w_i x_i \\\\\n",
        "-\\sum_{i=1}^n w_i x_i & \\sum_{i=1}^n w_i \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\\\n",
        "\n",
        "$$X^T W y = \\begin{bmatrix}\n",
        "1 & ... & 1 \\\\\n",
        "x_1 & ... & x_n \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "w_1 & 0 & 0 & ... & 0 \\\\\n",
        "0 & w_2 & 0 & ... & 0 \\\\\n",
        "\\ & \\ & \\ddots & \\ & \\ \\\\\n",
        "0 & ... & ... & ... & w_n \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "\\vdots \\\\\n",
        "y_n \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "1 & ... & 1 \\\\\n",
        "x_1 & ... & x_n \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "w_1 y_1 \\\\\n",
        "\\vdots \\\\\n",
        "w_n y_n \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "\\sum_{i=1}^n w_i y_i \\\\\n",
        "\\sum_{i=1}^n w_i y_i x_i\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$\\hat{\\beta} = (X^T W X)^{-1} X^T W y = \\frac{1}{\\sum_{i=1}^n w_i \\sum_{i=1}^n (w_i x_i)^2 - (\\sum_{i=1}^n w_i x_i)^2} \\begin{bmatrix}\n",
        "\\sum_{i=1}^n (w_i x_i)^2 & -\\sum_{i=1}^n w_i x_i \\\\\n",
        "-\\sum_{i=1}^n w_i x_i & \\sum_{i=1}^n w_i \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "\\sum_{i=1}^n w_i y_i \\\\\n",
        "\\sum_{i=1}^n w_i y_i x_i\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "\\frac{\\sum_{i=1}^n w_i y_i \\sum_{i=1}^n (w_i x_i)^2\n",
        "- \\sum_{i=1}^n w_i y_i x_i \\sum_{i=1}^n w_i x_i}\n",
        "{\\sum_{i=1}^n w_i \\sum_{i=1}^n (w_i x_i)^2 - (\\sum_{i=1}^n w_i x_i)^2} \\\\\n",
        "\\frac{-\\sum_{i=1}^n w_i y_i \\sum_{i=1}^n w_i x_i\n",
        "+ \\sum_{i=1}^n w_i y_i x_i \\sum_{i=1}^n w_i}\n",
        "{\\sum_{i=1}^n w_i \\sum_{i=1}^n (w_i x_i)^2 - (\\sum_{i=1}^n w_i x_i)^2} \\\\\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "\\hat{\\beta}_0 \\\\\n",
        "\\hat{\\beta}_1 \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Looking just at $\\hat{\\beta_1}$, when all $w_i = 1$, we get\n",
        "\n",
        "$$\\hat{\\beta}_1\n",
        "= \\frac{n \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}\n",
        "{n \\sum_{i=1}^n x_i^2 - (\\sum_{i=1}^n x_i)^2}\n",
        "= \\frac{\\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n}}\n",
        "{\\sum_{i=1}^n x_i^2 - \\frac{(\\sum_{i=1}^n x_i)^2}{n}}$$\n",
        "\n",
        "which is what we were meant to show."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m5JYwtH8A0I"
      },
      "source": [
        "## Question 2.4\n",
        "Show that the least-squares estimator of $β$ in the linear regression model is $\\hat{β} = (X^TWX)^{−1}X^TWy$, by following these steps.\n",
        "\n",
        "1. Show that $S = (y−Xβ)^TW(y−Xβ) = y^TWy−2β^T X^TWy+β^TX^TWXβ$. $S$ is the sum of the squared deviations.\n",
        "\n",
        "$$S = (y - X\\beta)^T W (y - X\\beta)$$\n",
        "$$ = y^T W y - y^T W X\\beta - (X\\beta)^T W y + (X\\beta)^T W X\\beta$$\n",
        "$$ = y^T W y - y^T W X\\beta - \\beta^TX^T W y + \\beta^TX^T W X\\beta$$\n",
        "$$ y^T W y - 2\\beta^TX^T W y + \\beta^TX^T W X\\beta$$\n",
        "\n",
        "2. Differentiate $S$ with respect to $β$ to find $dS/dβ$. (Hint: Differentiating $β^TMβ$ with respect to $β$ for any compatible matrix $M$ gives $2Mβ$.\n",
        "\n",
        "$$\\frac{dS}{d\\beta} = 0 - y^T W X - X^T W y + 2X^T W X\\beta = 2X^T W X\\beta - 2X^T W y$$\n",
        "\n",
        "3. Use the previous result to find the value of $\\hat{β}$ minimizing the value of $S$.\n",
        "\n",
        "$$\\frac{dS}{d\\beta} = 2X^T W X\\beta - 2X^T W y =_{set} 0$$\n",
        "$$\\rightarrow X^T W X\\beta = X^T W y$$\n",
        "$$\\hat{\\beta} = (X^T W X)^{-1} X^T W y$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohZAuA7c4u3N"
      },
      "source": [
        "## Question 2.5\n",
        "For simple linear regression, show that $\\hat{β}_1$ defined by (2.7)\n",
        "\n",
        "$$\\hat{\\beta_1} = \\frac{SS_{xy}}{SS_x} = \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)y_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}$$\n",
        "\n",
        "is an unbiased estimator of $β_1$. That is, show that $E[\\hat{β}_1] = β_1$. (Hint: $\\sum w_i(x_i −\\bar{x})a = 0$ for any constant $a$.)\n",
        "\n",
        "### Answer\n",
        "We can start with\n",
        "\n",
        "$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)y_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}$\n",
        "\n",
        "and substitute our linear model for $y_i$\n",
        "\n",
        "$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) (\\beta_0 + \\beta_1 x_i + \\epsilon_i)}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}$.\n",
        "\n",
        "Multiplying through in the numerator,\n",
        "\n",
        "$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\beta_0 + \\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\beta_1 x_i + \\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\epsilon_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}$.\n",
        "\n",
        "By the hint given in the problem, $\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\beta_0 = 0$ since $\\beta_0$ is a constant, so we have\n",
        "\n",
        "$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\beta_1 x_i + \\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\epsilon_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}\n",
        "= \\frac{\\beta_1 \\sum_{i=1}^n w_i(x_i - \\bar{x}_w) x_i + \\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\epsilon_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}\n",
        "= \\beta_1 \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) x_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2} + \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\epsilon_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}$.\n",
        "\n",
        "Now, by the linearity of expectation\n",
        "\n",
        "$E[\\hat{\\beta}_1] = E\\left[\\beta_1 \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) x_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2} \\right] + E\\left[\\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)\\epsilon_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2} \\right]$.\n",
        "\n",
        "Since $E[\\epsilon] = 0$, the second term drops out and we're left with\n",
        "\n",
        "$E[\\hat{\\beta}_1] = E\\left[\\beta_1  \\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) x_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2} \\right]$.\n",
        "\n",
        "We can further factor our $\\beta_1$ since it's a constant.\n",
        "\n",
        "$E[\\hat{\\beta}_1] = \\beta_1 E\\left[\\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) x_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2} \\right]$.\n",
        "\n",
        "The last part is tricky, but it turns out that $E\\left[\\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) x_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2} \\right] = 1$\n",
        "\n",
        "$\\frac{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w) x_i}{\\sum_{i=1}^n w_i(x_i - \\bar{x}_w)^2}\n",
        "= \\frac{\\sum{x_i^2 - x_i \\bar{x}}}{\\sum{x_i^2 - 2x_i \\bar{x} + \\bar{x}^2}}\n",
        "= \\frac{\\sum{x_i^2} - \\sum{\\bar{x} x_i}}{\\sum{x_i^2} - \\sum{2x_i \\bar{x}} + \\sum{\\bar{x}^2}}\n",
        "= \\frac{\\sum{x_i^2} - \\bar{x}\\sum{x_i}}{\\sum{x_i^2} - 2\\bar{x}\\sum{x_i} + n\\bar{x}^2}\n",
        "= \\frac{\\sum{x_i^2} - \\bar{x} n \\bar{x}}{\\sum{x_i^2} - 2\\bar{x} n \\bar{x} + n\\bar{x}^2}\n",
        "= \\frac{\\sum{x_i^2} - n\\bar{x}^2}{\\sum{x_i^2} - 2n\\bar{x}^2 + n\\bar{x}^2}\n",
        "= \\frac{\\sum{x_i^2} - n\\bar{x}^2}{\\sum{x_i^2} - n\\bar{x}^2}\n",
        "= 1$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$E[\\hat{\\beta}_1] = \\beta_1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srcQtFBrOsgV"
      },
      "source": [
        "## Question 2.6\n",
        "Show that $\\hat{β} = (X^TWX)^{−1}X^TWy$ is an unbiased estimator of $β$. That is, show $E[\\hat{β}] = β$.\n",
        "\n",
        "### Answer\n",
        "Start by taking the expectation of both sides:\n",
        "\n",
        "$E[\\hat{\\beta}] = E[(X^T W X)^{-1} X^T W y]$.\n",
        "\n",
        "Since $X$ and $W$ are matrices with constant values, they can be taken outside of the $E[]$ operator, leaving\n",
        "\n",
        "$E[(X^T W X)^{-1} X^T W y] = (X^T W X)^{-1} X^T W E[y]$.\n",
        "\n",
        "Now, since $E[y] = X\\beta$, we get\n",
        "\n",
        "$E[\\hat{\\beta}] = (X^T W X)^{-1} X^T W X\\beta$.\n",
        "\n",
        "If $X^TWX$ is invertible (which means, for a matrix $M$, that $M^{-1}M = I$, where $I$ is the identity matrix), which it must be for least squares, then\n",
        "\n",
        "$E[\\hat{\\beta}] = (X^T W X)^{-1} (X^T W X) \\beta = I \\beta = \\beta$\n",
        "\n",
        "where $X^TWX$ are acting as $M$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBFdQAFgBqmz"
      },
      "source": [
        "## Question 2.7\n",
        "Show that the variance–covariance matrix of $\\hat{β}$ is $var[\\hat{β}] = (X^T WX)^{−1}σ^2$, using that $var[Cy] = Cvar[y]C^T$ for a constant matrix $C$.\n",
        "\n",
        "### Answer\n",
        "Start with the expression for $\\hat{\\beta}$.\n",
        "\n",
        "$\\hat{β} = (X^TWX)^{−1}X^TWy = (X^TWX)^{−1}X^TW (X\\beta + \\epsilon)\n",
        "= (X^TWX)^{−1}X^TW X\\beta + (X^TWX)^{−1}X^TW \\epsilon$\n",
        "\n",
        "\\\n",
        "\n",
        "Since $(X^TWX)^{−1}X^TWX = I$, we get\n",
        "\n",
        "$\\hat{β} = I\\beta + (X^TWX)^{−1}X^TW \\epsilon\n",
        "= \\beta + (X^TWX)^{−1}X^TW \\epsilon$\n",
        "\n",
        "\\\n",
        "\n",
        "Now take the variance of both sides\n",
        "\n",
        "$var[\\hat{β}] = var[\\beta + (X^TWX)^{−1}X^TW \\epsilon] =\n",
        "var[\\beta] + var[(X^TWX)^{−1}X^TW \\epsilon]$\n",
        "\n",
        "\\\n",
        "\n",
        "Since the varaince of a constant is 0, $var[\\beta]=0$ such that\n",
        "\n",
        "$var[\\hat{β}] = var[(X^TWX)^{−1}X^TW \\epsilon]$\n",
        "\n",
        "\\\n",
        "\n",
        "Using the hint in the question, since $(X^TWX)^{−1}X^TW$ is constant like $C$,\n",
        "\n",
        "$var[\\hat{β}] = (X^TWX)^{−1}X^TW \\ var[\\epsilon] \\ ((X^TWX)^{−1}X^TW)^T$\n",
        "\n",
        "\\\n",
        "\n",
        "Taking $var[\\epsilon] = \\sigma^2 W^{-1}$ and reworking the transposes\n",
        "\n",
        "$var[\\hat{\\beta}] = (X^TWX)^{−1}X^TW (\\sigma^2 W^{-1}) (X^T W)^T((X^TWX)^{−1})^T\n",
        "= (X^TWX)^{−1}X^TW (\\sigma^2 W^{-1}) (W^T X) ((X^T W X)^T)^{-1}\n",
        "= (X^TWX)^{−1}X^TW (\\sigma^2 W^{-1}) (W^T X) (X^T W^T X)^{-1}$\n",
        "\n",
        "\\\n",
        "\n",
        "Since $\\sigma^2$ is a scalar, we can take it out front.\n",
        "\n",
        "$var[\\hat{\\beta}] = \\sigma^2 (X^TWX)^{−1}X^TW (W^{-1}) (W^T X) (X^T W^T X)^{-1}$\n",
        "\n",
        "\\\n",
        "\n",
        "Since $W W^{-1} = I$\n",
        "\n",
        "$var[\\hat{\\beta}] = \\sigma^2 (X^TWX)^{−1}X^T \\ I \\ (W^T X) (X^T W^T X)^{-1}\n",
        "= \\sigma^2 (X^TWX)^{−1}X^T (W^T X) (X^T W^T X)^{-1}$\n",
        "\n",
        "\\\n",
        "\n",
        "Now we just rearrange some parentheses to make it obvious\n",
        "\n",
        "$var[\\hat{\\beta}] = \\sigma^2 (X^TWX)^{−1}X^T (W^T X) (X^T W^T X)^{-1}\n",
        "= \\sigma^2 (X^TWX)^{−1} (X^T W^T X) (X^T W^T X)^{-1}$\n",
        "\n",
        "\\\n",
        "\n",
        "Since $(X^T W^T X) (X^T W^T X)^{-1} = I$\n",
        "\n",
        "$var[\\hat{\\beta}] = \\sigma^2 (X^TWX)^{−1} I = \\sigma^2 (X^TWX)^{−1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAdw-FKGFbdh"
      },
      "source": [
        "## Question 2.8\n",
        "Show that the $F$-statistic (2.28)\n",
        "\n",
        "$$F = \\frac{_{SS}R_{EG} / (p' - 1)}{RSS / (n - p')} = \\frac{_{MS}R_{EG}}{MSE}$$\n",
        "\n",
        "and $R^2$ (2.29)\n",
        "\n",
        "$$R^2 = \\frac{_{SS}R_{EG}}{SST} = 1 - \\frac{RSS}{SST}$$\n",
        "\n",
        "are related by\n",
        "$$F = \\frac{R^2 / (p' - 1)}{(1 - R^2) / (n - p')}$$\n",
        "\n",
        "### Answer\n",
        "I'm not really sure what is being asked here.  The final equation given in the question already *does* show how $F$ and $R^2$ relate.  But the solution in the back of the book says\n",
        "> \"Substituting for $R^2$ on the right in terms of $SS$ gives $\\frac{ssReg/(p' −1)}{SST/(n−p')}$, which is $F.$\"\n",
        "\n",
        "So fair enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "544ump1CvLDG"
      },
      "source": [
        "## Question 2.9\n",
        "Consider a simple linear regression model with systematic component $μ = β_0 + β_1 x$. Suppose we wish to design an experiment with $n = 5$ observations, when $σ^2$ is known to be $1$. Suppose three designs for the experiment are considered. In Design A, the values of the explanatory variable are $x = [1, 1, −1, −1, 0]$. In Design B, the values are $x = [1, 1, 1, 1, −1]$. In Design C, the values are $x = [1, 0.5, 0, −0.5, −1]$.\n",
        "\n",
        "1. Write the model matrix $X$ for each design.\n",
        "2. Compute $var[\\hat{μ}]$ for each design.\n",
        "3. Plot $var[\\hat{μ}]$ for $x_g$ between −1 and 1. When would Design A be preferred, and why? When would Design B be preferred, and why? When would Design C be preferred, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0OKd3F5wI6Z"
      },
      "source": [
        "### Answer to 2.9, 1\n",
        "***Write a model matrix $X$ for each design***.\n",
        "\n",
        "$$X = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_5 \\\\\n",
        "\\end{bmatrix}$$\n",
        "\\\n",
        "$$X_A = \\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 1 \\\\\n",
        "1 & -1 \\\\\n",
        "1 & -1 \\\\\n",
        "1 & 0 \\\\\n",
        "\\end{bmatrix}$$\n",
        "\\\n",
        "$$X_B = \\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 1 \\\\\n",
        "1 & 1 \\\\\n",
        "1 & 1 \\\\\n",
        "1 & -1 \\\\\n",
        "\\end{bmatrix}$$\n",
        "\\\n",
        "$$X_C = \\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 0.5 \\\\\n",
        "1 & 0 \\\\\n",
        "1 & -0.5 \\\\\n",
        "1 & -1 \\\\\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKE84JCbxFg8"
      },
      "source": [
        "### Answer to 2.9, 2\n",
        "***Compute $var[\\hat{μ}]$ for each design.***\n",
        "\n",
        "$var[\\hat{\\mu}] = x(X^T W X)^{-1} x^T \\sigma^2$\n",
        "\n",
        "(here, $\\sigma^2 = 1$)\n",
        "\n",
        "$var[\\hat{\\mu}_A] =\n",
        "\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix} \\left(\n",
        "  \\begin{bmatrix}\n",
        "  1 & 1 & 1 & 1 & 1 \\\\\n",
        "  1 & 1 & -1 & -1 & 0 \\\\\n",
        "  \\end{bmatrix}\n",
        "  \\begin{bmatrix} I \\end{bmatrix}\n",
        "  \\begin{bmatrix}\n",
        "    1 & 1 \\\\\n",
        "    1 & 1 \\\\\n",
        "    1 & -1 \\\\\n",
        "    1 & -1 \\\\\n",
        "    1 & 0 \\\\\n",
        "\\end{bmatrix}\\right)^{-1}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}\n",
        "1$\n",
        "\n",
        ">> $=\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix} \\left(\n",
        "  \\begin{bmatrix}\n",
        "  5 & 0\\\\\n",
        "  0 & 4\\\\\n",
        "\\end{bmatrix}\\right)^{-1}\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        ">> $=\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix}\n",
        "  \\begin{bmatrix}\n",
        "  0.2 & 0\\\\\n",
        "  0 & 0.25\\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        ">> $= 0.2 + 0.25x^2$\n",
        "\n",
        ">> *The book says $var[\\hat{\\mu}_A] = 0.2x^2 + 0.25$, which you would get if you had $(X^T W X)^{-1} = \\begin{bmatrix}\n",
        "  0.25 & 0\\\\\n",
        "  0 & 0.20\\\\\n",
        "\\end{bmatrix}$, which I think is incorrect.*\n",
        "\n",
        "\\\n",
        "\n",
        "$var[\\hat{\\mu}_B] =\n",
        "\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix} \\left(\n",
        "  \\begin{bmatrix}\n",
        "  1 & 1 & 1 & 1 & 1 \\\\\n",
        "  1 & 1 & 1 & 1 & -1 \\\\\n",
        "  \\end{bmatrix}\n",
        "  \\begin{bmatrix} I \\end{bmatrix}\n",
        "  \\begin{bmatrix}\n",
        "    1 & 1 \\\\\n",
        "    1 & 1 \\\\\n",
        "    1 & 1 \\\\\n",
        "    1 & 1 \\\\\n",
        "    1 & -1 \\\\\n",
        "\\end{bmatrix}\\right)^{-1}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}\n",
        "1$\n",
        "\n",
        ">> $=\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix} \\left(\n",
        "  \\begin{bmatrix}\n",
        "  5 & 3\\\\\n",
        "  3 & 5\\\\\n",
        "\\end{bmatrix}\\right)^{-1}\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        ">> $=\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix} \\left(\n",
        "  \\frac{1}{16}\n",
        "  \\begin{bmatrix}\n",
        "  5 & -3\\\\\n",
        "  -3 & 5\\\\\n",
        "\\end{bmatrix} \\right)\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        ">> $= \\frac{5x^2 - 6x + 5}{16}$\n",
        "\n",
        "\\\n",
        "\n",
        "$var[\\hat{\\mu}_C] =\n",
        "\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix} \\left(\n",
        "  \\begin{bmatrix}\n",
        "  1 & 1 & 1 & 1 & 1 \\\\\n",
        "  1 & 0.5 & 0 & -0.5 & -1 \\\\\n",
        "  \\end{bmatrix}\n",
        "  \\begin{bmatrix} I \\end{bmatrix}\n",
        "  \\begin{bmatrix}\n",
        "    1 & 1 \\\\\n",
        "    1 & 0.5 \\\\\n",
        "    1 & 0 \\\\\n",
        "    1 & -0.5 \\\\\n",
        "    1 & -1 \\\\\n",
        "\\end{bmatrix}\\right)^{-1}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}\n",
        "1$\n",
        "\n",
        ">> $=\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix} \\left(\n",
        "  \\begin{bmatrix}\n",
        "  5 & 0\\\\\n",
        "  0 & 2.5\\\\\n",
        "\\end{bmatrix}\\right)^{-1}\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        ">> $=\\begin{bmatrix}\n",
        "1 & x\\\\\n",
        "\\end{bmatrix} \\left(\n",
        "  \\frac{1}{12.5}\n",
        "  \\begin{bmatrix}\n",
        "  2.5 & 0\\\\\n",
        "  0 & 5\\\\\n",
        "\\end{bmatrix} \\right)\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "x\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        ">> $= \\frac{2.5 + 5x^2}{12.5} = \\frac{1 + 2x^2}{5}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4UdsyjsxTz4"
      },
      "source": [
        "### Answer to 2.9, 3\n",
        "***Plot $var[\\hat{μ}]$ for $x_g$ between −1 and 1. When would Design A be preferred, and why? When would Design B be preferred, and why? When would Design C be preferred, and why?***\n",
        "\n",
        "I'll use the code from the back of the book to plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "ZNfM3XnncMHL",
        "outputId": "25cb5d40-3feb-497b-e5dd-33f5cbcd8be2",
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "plot without title"
            ]
          },
          "metadata": {
            "image/png": {
              "height": 420,
              "width": 420
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x <- seq(-1, 1, length=100)\n",
        "varA <- function(x){0.2 + 0.25*x^2}\n",
        "varB <- function(x){(5 - 6*x + 5*x^2)/16}\n",
        "varC <- function(x){(1+2*x^2)/5}\n",
        "vA <- varA(x); vB <- varB(x); vC <- varC(x)\n",
        "plot( range(c(vA, vB, vC)) ~ range(x), type=\"n\", ylim=c(0, 1.2), ylab=\"Var. of predictions\", xlab=\"x values\", las=1)\n",
        "lines(varA(x) ~ x, lty=1, lwd=2)\n",
        "lines(varB(x) ~ x, lty=2, lwd=2)\n",
        "lines(varC(x) ~ x, lty=3, lwd=2)\n",
        "legend(\"top\", lwd=2, lty=1:3, legend=c(\"Design A\", \"Design B\", \"Design C\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z025-ZM_c6sw"
      },
      "source": [
        "The times that Design A, B, or C would be preferred corresponds to the domain (x-values) over which their representative curves are lower than other 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEppRNX0cv25"
      },
      "source": [
        "## Question 2.10\n",
        "Assume that a quantitative response variable $y$ and a covariate $x$ are related by some smooth function $f$ such that $μ = f(x)$ where $μ = E[y]$.\n",
        "\n",
        "1. Assuming that the necessary derivatives exist, find the first-order Taylor series expansion of $f(x)$ expanded about $\\bar{x}$, where $\\bar{x}$ is the mean of $x$.\n",
        "2. Rearrange this expression into the form of a multiple regression model.\n",
        "3. Explain how this implies that regression models are locally linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-VneCLNr4GX"
      },
      "source": [
        "### Answer to 2.10, 1\n",
        "***Assuming that the necessary derivatives exist, find the first-order Taylor series expansion of $f(x)$ expanded about $\\bar{x}$, where $\\bar{x}$ is the mean of $x$.***\n",
        "\n",
        "If you're not familiar with Taylor series, you might want to check out [Khan Academy's lesson](https://www.khanacademy.org/math/ap-calculus-bc/bc-series-new/bc-10-11/v/maclaurin-and-taylor-series-intuition).  Taylor series are a way of expressing any smooth, continuous function with a polynomial series.  Generically, a Taylor series looks like:\n",
        "\n",
        "$f(x) = \\sum_{n=0}^\\infty \\frac{f^{(n)}(k)}{n!} (x-k)^n$\n",
        "\n",
        "> $= \\frac{f^{(0)}(k)}{0!} (x-k)^0 + \\frac{f^{(1)}(k)}{1!} (x-k)^1 + \\frac{f^{(2)}(k)}{2!} (x-k)^2 + \\frac{f^{(3)}(k)}{3!} (x-k)^3 + ...$\n",
        "> $= f(k) + f'(k)(x-k) + \\frac{f''(a)}{2}(x-k)^2 + \\frac{f'''(k)}{3}(x-k)^3 + ...$\n",
        "\n",
        "where $k$ is the value of $x$ for which we are evaluating $f(x)$, $f(x=k)$.  \n",
        "\n",
        "Of course, we don't generally want to find an infinity of terms, so we so just find a few and accept the truncated expression as an approximation of $f(x)$ like\n",
        "\n",
        "$f(x) \\approx f(k) + f'(k)(x-k) + \\frac{f''(k)}{2} (x-k)^2$\n",
        "\n",
        "In the context of our question, we have\n",
        "\n",
        "$\\mu = f(x) = f(\\bar{x}) + f'(\\bar{x})(x - \\bar{x}) + \\frac{f''(\\bar{x})}{2} (x - \\bar{x})^2 + ...$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9gzEnQnyEG_"
      },
      "source": [
        "### Answer to 2.10, 2\n",
        "***Rearrange this expression into the form of a multiple regression model.***\n",
        "\n",
        "There is more than one way to do this.  My first inclination is this.  Firstly, notice that our Taylor expansion *is* a linear function, where the $f(\\bar{x})^{(n)}$ terms are like the $\\beta$ coefficients and the $(x-\\bar{x})^n$ terms are the like the data $z$ values.  So we could simply change the notation such that\n",
        "\n",
        "$\\mu = f(x) = f(\\bar{x}) + f'(\\bar{x})(x - \\bar{x}) + \\frac{f''(\\bar{x})}{2} (x - \\bar{x})^2 + ...$\n",
        ">$=\\beta_0 + \\beta_1 z_1 + \\beta_2 z_2 + ...$\n",
        "\n",
        "However, the back-of-the-book answer is \"$f(x)$ is linear in $x$ if $x-\\bar{x}$ is small.\"  So apparently the author wants us to recognize that $f(x)$ is (approximately) linear in $x$ rather than $x-\\bar{x}$.  *Why is $f(x)$ linear in $x$ only if $x-\\bar{x}$ is small?* When $x-\\bar{x}$, say $|x-\\bar{x}| \\le 1$, then $f(x)$ is just being approximated by proper fractions of the $f(\\bar{x})^{(n)}$ terms.  But as $|x-\\bar{x}| \\ge 1$, the $(x-\\bar{x})^n$ start to multiply the $f(\\bar{x})^{(n)}$ terms by large values as $n \\ge 2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_T4opgCzmGw"
      },
      "source": [
        "### Answer to 2.10, 3\n",
        "***Explain how this implies that regression models are locally linear.***\n",
        "\n",
        "Since, when $x-\\bar{x}$ is small, the Taylor approximation of $f(x)$ is linear, the the *local* domain around $k=\\bar{x}$ is (approximately) linear.\n",
        "\n",
        "I suppose that the reason this is important is because we will often find/presume relations between $y$ and $x$ that are not linear across the domain of $x$.  However, we can rely on *local linearity* to have some confidence about some inferences of our linear models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
